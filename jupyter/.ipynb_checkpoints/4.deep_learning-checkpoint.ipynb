{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "# Path to train and test files\n",
    "train_path = '../data/train_E6oV3lV.csv'\n",
    "test_path = '../data/test_tweets_anuFYb8.csv'\n",
    "\n",
    "train  = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "# Processed data\n",
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(all_data['tidy_tweet'])\n",
    "\n",
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_data['tidy_tweet'])\n",
    "\n",
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "# Stack BOW and TF-IDF\n",
    "x_val = hstack([xvalid_tfidf,xvalid_bow])\n",
    "x_train = hstack([xtrain_tfidf,xtrain_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373, 833)\n",
      "(22373,)\n",
      "(9589, 833)\n",
      "(9589,)\n"
     ]
    }
   ],
   "source": [
    "# Merge BOW and TF-IDF and aply PCA\n",
    "\n",
    "x_train = x_train.toarray()\n",
    "x_test = x_val.toarray()\n",
    "y_train = ytrain\n",
    "y_test = yvalid\n",
    "\n",
    "pca = decomposition.PCA(n_components=0.95, svd_solver='full')\n",
    "pca.fit(x_train)\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_mayusculas = count_caps(all_data['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(conteo_mayusculas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers import GaussianNoise as GN\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "from personal_library.sce_keras.callbacks import (\n",
    "    F1PrecisionRecall,\n",
    "    LearningRateDecay\n",
    ")\n",
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "learnRate = 0.001\n",
    "\n",
    "metric = F1PrecisionRecall()\n",
    "lrate_decay = LearningRateDecay(epochs, learnRate).poly_decay\n",
    "\n",
    "#Define model architecture\n",
    "model = Sequential()\n",
    "model.add( Dense( 2048, activation='relu', input_shape=(x_train.shape[1],) ) )\n",
    "model.add(BN())\n",
    "model.add(GN(0.3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(BN())\n",
    "model.add(GN(0.3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BN())\n",
    "model.add(GN(0.3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BN())\n",
    "model.add(GN(0.3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "#model.summary()\n",
    "\n",
    "checkpoint_path = \"../model_wehigts/Wehigts.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=1,\n",
    "                               save_best_only=True, mode='min')\n",
    "\n",
    "adam = Adam(lr=learnRate, beta_1=0.9, beta_2=0.999,\n",
    "            epsilon=None, decay=1e-6, amsgrad=False)\n",
    "rms = RMSprop(lr=learnRate, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss=f1_loss, \n",
    "            optimizer=adam, \n",
    "            metrics=['accuracy', f1]) \n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[checkpointer, metric, lrate_decay])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network f1_sklearn: 0.34768740031897927\n"
     ]
    }
   ],
   "source": [
    "#Load best model\n",
    "model.load_weights(checkpoint_path)\n",
    "y_pred = model.predict(x_test, batch_size=1)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Neural Network f1_sklearn: {}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
