{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Numberbatch or Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiancorrea/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/sebastiancorrea/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']==1.0) | (all_data['label']==0.0)]\n",
    "y = all_data['label'][(all_data['label']==1.0) | (all_data['label']==0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569, 19) (25569,)\n",
      "(6393, 19) (6393,)\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(full_text)\n",
    "max_len = 19\n",
    "X = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = '../data/vectors/glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "embed_size = 50\n",
    "\n",
    "max_features = 30000\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 19, 50)       758350      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 19, 50)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 19, 256)      137472      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 16, 32)       32800       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 16, 256)      164864      conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16, 32)       32800       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 13, 32)       32800       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 13, 32)       32800       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 32)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 32)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 32)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 32)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 32)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 32)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16448       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           256         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            33          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,211,727\n",
      "Trainable params: 452,737\n",
      "Non-trainable params: 758,990\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/10\n",
      "25569/25569 [==============================] - 60s 2ms/step - loss: 0.7459 - acc: 0.8039 - f1: 0.2541 - val_loss: 0.7481 - val_acc: 0.9215 - val_f1: 0.2519\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74813, saving model to ../model_wehigts/lstm_glove_w.hdf5\n",
      "Epoch 2/10\n",
      "25569/25569 [==============================] - 55s 2ms/step - loss: 0.7227 - acc: 0.8688 - f1: 0.2773 - val_loss: 0.7357 - val_acc: 0.9127 - val_f1: 0.2643\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74813 to 0.73570, saving model to ../model_wehigts/lstm_glove_w.hdf5\n",
      "Epoch 3/10\n",
      "25569/25569 [==============================] - 55s 2ms/step - loss: 0.7326 - acc: 0.8454 - f1: 0.2674 - val_loss: 0.8185 - val_acc: 0.5124 - val_f1: 0.1815\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.73570\n",
      "Epoch 4/10\n",
      "25569/25569 [==============================] - 55s 2ms/step - loss: 0.6937 - acc: 0.8797 - f1: 0.3063 - val_loss: 0.6909 - val_acc: 0.8907 - val_f1: 0.3091\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.73570 to 0.69094, saving model to ../model_wehigts/lstm_glove_w.hdf5\n",
      "Epoch 5/10\n",
      "25569/25569 [==============================] - 55s 2ms/step - loss: 0.7040 - acc: 0.8616 - f1: 0.2960 - val_loss: 0.7245 - val_acc: 0.9191 - val_f1: 0.2755\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69094\n",
      "Epoch 6/10\n",
      "25569/25569 [==============================] - 55s 2ms/step - loss: 0.7137 - acc: 0.8592 - f1: 0.2863 - val_loss: 0.7474 - val_acc: 0.9235 - val_f1: 0.2526\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69094\n",
      "Epoch 7/10\n",
      "25569/25569 [==============================] - 56s 2ms/step - loss: 0.6927 - acc: 0.8740 - f1: 0.3073 - val_loss: 0.7182 - val_acc: 0.8753 - val_f1: 0.2818\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69094\n",
      "Epoch 8/10\n",
      "25569/25569 [==============================] - 56s 2ms/step - loss: 0.6997 - acc: 0.8735 - f1: 0.3003 - val_loss: 0.7852 - val_acc: 0.9260 - val_f1: 0.2148\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69094\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25569/25569 [==============================] - 51s 2ms/step - loss: 0.6963 - acc: 0.8815 - f1: 0.3037 - val_loss: 0.7023 - val_acc: 0.9074 - val_f1: 0.2977\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69094\n",
      "Epoch 10/10\n",
      "25569/25569 [==============================] - 51s 2ms/step - loss: 0.6935 - acc: 0.8874 - f1: 0.3065 - val_loss: 0.6852 - val_acc: 0.9157 - val_f1: 0.3148\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.69094 to 0.68517, saving model to ../model_wehigts/lstm_glove_w.hdf5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learnRate = 0.00001\n",
    "checkpoint_path = \"../model_wehigts/lstm_glove_w.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=1,\n",
    "                               save_best_only=True, mode='min')\n",
    "units = 128\n",
    "kernel_size1 = 4\n",
    "spatial_dr = 0.5\n",
    "kernel_size2 = 4\n",
    "dense_units = 64 \n",
    "dr=0.2\n",
    "conv_size=32\n",
    "\n",
    "\n",
    "inp = Input(shape = (max_len,))\n",
    "x = Embedding(nb_words+1, embed_size, weights = [embedding_matrix], trainable=False)(inp)\n",
    "x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "x_gru = Bidirectional(GRU(units, return_sequences=True))(x1)\n",
    "x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "x_lstm = Bidirectional(LSTM(units, return_sequences=True))(x1)\n",
    "x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "x = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()\n",
    "adam = Adam(lr=learnRate, beta_1=0.9, beta_2=0.999,\n",
    "            epsilon=None, decay=1e-6, amsgrad=False)\n",
    "\n",
    "# 'binary_crossentropy'\n",
    "model.compile(loss=f1_loss, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', f1]) \n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
