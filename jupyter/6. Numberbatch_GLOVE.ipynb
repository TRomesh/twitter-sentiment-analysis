{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Numberbatch or Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']==1.0) | (all_data['label']==0.0)]\n",
    "y = all_data['label'][(all_data['label']==1.0) | (all_data['label']==0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(full_text)\n",
    "max_len = 19\n",
    "X = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = '../data/vectors/glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "embed_size = 50\n",
    "\n",
    "max_features = 30000\n",
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learnRate = 0.00001\n",
    "checkpoint_path = \"../model_wehigts/7_w.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=True, mode='min')\n",
    "units = 128\n",
    "kernel_size1 = 4\n",
    "spatial_dr = 0.5\n",
    "kernel_size2 = 4\n",
    "dense_units = 64 \n",
    "dr=0.2\n",
    "conv_size=32\n",
    "\n",
    "\n",
    "inp = Input(shape = (max_len,))\n",
    "x = Embedding(nb_words+1, embed_size, weights = [embedding_matrix], trainable=False)(inp)\n",
    "x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "x_gru = Bidirectional(GRU(units, return_sequences=True))(x1)\n",
    "x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "x_lstm = Bidirectional(LSTM(units, return_sequences=True))(x1)\n",
    "x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "x = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()\n",
    "adam = Adam(lr=learnRate, beta_1=0.9, beta_2=0.999,\n",
    "            epsilon=None, decay=1e-6, amsgrad=False)\n",
    "\n",
    "# 'binary_crossentropy'\n",
    "model.compile(loss=f1_loss, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', f1]) \n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load best model\n",
    "model.load_weights(checkpoint_path)\n",
    "y_pred = model.predict(x_val, batch_size=1)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Neural Network f1_sklearn: {}\".format(f1_score(y_val, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
