{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    LSTM,\n",
    "    Embedding,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']=='1.0') | (all_data['label']=='0.0')]\n",
    "y = all_data['label'][(all_data['label']=='1.0') | (all_data['label']=='0.0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569, 1) (25569,)\n",
      "(6393, 1) (6393,)\n"
     ]
    }
   ],
   "source": [
    "max_len = 120\n",
    "\n",
    "texts = full_text.tolist()\n",
    "texts = [' '.join(t.split()[:max_len]) for t in texts]\n",
    "texts = np.array(texts, dtype=object)[:, np.newaxis]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(texts, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 16:28:12.116470 140736058794880 deprecation_wrapper.py:119] From ../personal_library/NLP/tokenizers/bert_token.py:16: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0709 16:28:20.006681 140736058794880 deprecation_wrapper.py:119] From ../personal_library/NLP/bert/tokenization.py:126: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6c4b06e00f4200a4230f5f7ab9c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25569, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b297c469a8204d7eaf19ddb599d6cde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=6393, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from personal_library.NLP.tokenizers.bert_token import (\n",
    "    create_tokenizer_from_hub_module,\n",
    "    convert_text_to_examples,\n",
    "    convert_examples_to_features\n",
    ")\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(x_train, y_train)\n",
    "test_examples = convert_text_to_examples(x_val, y_val)\n",
    "\n",
    "# Convert to features\n",
    "train_input_ids, train_input_masks, train_segment_ids, train_labels = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_len)\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_labels = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 768)          0           bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 22,051,329\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/2\n",
      "   64/25569 [..............................] - ETA: 4:48:35 - loss: 0.4974 - acc: 0.8594 - f1: 0.1175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "from personal_library.sce_keras.layers.bert import BertLayer\n",
    "from personal_library.sce_keras.callbacks import (\n",
    "    LearningRateDecay,\n",
    "    WarmUpCosineDecayScheduler\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "learnRate = 0.001\n",
    "lstm_out = 200\n",
    "warmup_epoch = 1\n",
    "\n",
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=learnRate,\n",
    "                                        warmup_learning_rate=0,\n",
    "                                        warmup_epoch=warmup_epoch,\n",
    "                                        hold_base_rate_steps=5,\n",
    "                                        verbose=0)\n",
    "\n",
    "checkpoint_path = \"../model_wehigts/9_w.hdf5\"\n",
    "checkpoint_path1 = \"../model_wehigts/9_ch.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=True, mode='min')\n",
    "checkpointer1 = ModelCheckpoint(filepath=checkpoint_path1,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=False, mode='min')\n",
    "\n",
    "# Create inputs\n",
    "in_id = Input(shape=(max_len,), name=\"input_ids\")\n",
    "in_mask = Input(shape=(max_len,), name=\"input_masks\")\n",
    "in_segment = Input(shape=(max_len,), name=\"segment_ids\")\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "x = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=bert_inputs, outputs=x)\n",
    "model.summary()\n",
    "\n",
    "# 'binary_crossentropy'\n",
    "model.compile(loss='binary_crossentropy',#f1_loss, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', f1])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "history = model.fit([train_input_ids, train_input_masks, train_segment_ids], train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "                    callbacks=[checkpointer, checkpointer1, warm_up_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "#Load best model\n",
    "model.load_weights(checkpoint_path)\n",
    "y_pred = model.predict(x_val, batch_size=1)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Own emmbeding f1_sklearn: {}\".format(f1_score(y_val.astype(float), y_pred)))\n",
    "print(\"Own emmbeding accuracy: {}\".format(accuracy_score(y_val.astype(float), y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
