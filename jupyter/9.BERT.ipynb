{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    LSTM,\n",
    "    Embedding,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']=='1.0') | (all_data['label']=='0.0')]\n",
    "y = all_data['label'][(all_data['label']=='1.0') | (all_data['label']=='0.0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569, 1) (25569,)\n",
      "(6393, 1) (6393,)\n"
     ]
    }
   ],
   "source": [
    "max_len = 120\n",
    "\n",
    "texts = full_text.tolist()\n",
    "texts = [' '.join(t.split()[:max_len]) for t in texts]\n",
    "texts = np.array(texts, dtype=object)[:, np.newaxis]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(texts, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 19:51:16.495263 140269782828800 deprecation_wrapper.py:119] From ../personal_library/NLP/tokenizers/bert_token.py:16: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0712 19:51:20.040235 140269782828800 deprecation_wrapper.py:119] From ../personal_library/NLP/bert/tokenization.py:126: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c3b471303349c788bd926c113be04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=25569, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53156e1d4d24803903aa80d2ee9bab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=6393, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from personal_library.NLP.tokenizers.bert_token import (\n",
    "    create_tokenizer_from_hub_module,\n",
    "    convert_text_to_examples,\n",
    "    convert_examples_to_features\n",
    ")\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(x_train, y_train)\n",
    "test_examples = convert_text_to_examples(x_val, y_val)\n",
    "\n",
    "# Convert to features\n",
    "train_input_ids, train_input_masks, train_segment_ids, train_labels = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_len)\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_labels = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 19:51:29.960994 140269782828800 deprecation.py:506] From /home/scorrea/anaconda3/envs/sebasenv/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0712 19:51:30.091378 140269782828800 deprecation.py:323] From /home/scorrea/anaconda3/envs/sebasenv/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 120)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 22,051,329\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/200\n",
      "  128/25569 [..............................] - ETA: 3:37:22 - loss: 0.8756 - acc: 0.2500 - f1: 0.0640"
     ]
    }
   ],
   "source": [
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "from personal_library.sce_keras.layers.bert import BertLayer\n",
    "from personal_library.sce_keras.callbacks import (\n",
    "    LearningRateDecay,\n",
    "    WarmUpCosineDecayScheduler\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "learnRate = 0.001\n",
    "lstm_out = 200\n",
    "warmup_epoch = 20\n",
    "\n",
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=learnRate,\n",
    "                                        warmup_learning_rate=0,\n",
    "                                        warmup_epoch=warmup_epoch,\n",
    "                                        hold_base_rate_steps=5,\n",
    "                                        verbose=0)\n",
    "\n",
    "checkpoint_path = \"../model_wehigts/9_w.hdf5\"\n",
    "checkpoint_path1 = \"../model_wehigts/9_ch.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=True, mode='min')\n",
    "checkpointer1 = ModelCheckpoint(filepath=checkpoint_path1,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=False, mode='min')\n",
    "\n",
    "# Create inputs\n",
    "in_id = Input(shape=(max_len,), name=\"input_ids\")\n",
    "in_mask = Input(shape=(max_len,), name=\"input_masks\")\n",
    "in_segment = Input(shape=(max_len,), name=\"segment_ids\")\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "x = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=bert_inputs, outputs=x)\n",
    "model.summary()\n",
    "\n",
    "# 'binary_crossentropy'\n",
    "model.compile(loss='binary_crossentropy',#f1_loss, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', f1])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "history = model.fit([train_input_ids, train_input_masks, train_segment_ids], train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "                    callbacks=[checkpointer, checkpointer1, warm_up_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "#Load best model\n",
    "model.load_weights(checkpoint_path)\n",
    "y_pred = model.predict(x_val, batch_size=1)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Own emmbeding f1_sklearn: {}\".format(f1_score(y_val.astype(float), y_pred)))\n",
    "print(\"Own emmbeding accuracy: {}\".format(accuracy_score(y_val.astype(float), y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
