{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/sebastiancorrea/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    LSTM,\n",
    "    Embedding,\n",
    "    Dropout,\n",
    "    Activation,\n",
    "    SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']==1.0) | (all_data['label']==0.0)]\n",
    "y = all_data['label'][(all_data['label']==1.0) | (all_data['label']==0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embeddings\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('../data/vectors/cc.en.300.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(full_text)\n",
    "word_index = tk.word_index\n",
    "\n",
    "X = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "from personal_library.sce_keras.callbacks import (\n",
    "    LearningRateDecay,\n",
    "    WarmUpCosineDecayScheduler\n",
    ")\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "learnRate = 0.001\n",
    "lstm_out = 200\n",
    "warmup_epoch = 20\n",
    "\n",
    "lrate_decay = LearningRateDecay(epochs, learnRate).step_decay\n",
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=learnRate,\n",
    "                                        warmup_learning_rate=0,\n",
    "                                        warmup_epoch=warmup_epoch,\n",
    "                                        hold_base_rate_steps=5,\n",
    "                                        verbose=0)\n",
    "\n",
    "checkpoint_path = \"../model_wehigts/6_w.hdf5\"\n",
    "checkpoint_path1 = \"../model_wehigts/6_ch.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=True, mode='min')\n",
    "checkpointer1 = ModelCheckpoint(filepath=checkpoint_path1,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=False, mode='min')\n",
    "\n",
    "inp = Input(shape = (max_len,))\n",
    "x = Embedding(nb_words+1, embed_size, weights = [embedding_matrix], trainable=False)(inp)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = LSTM(lstm_out, dropout=0.5, recurrent_dropout=0.5)(x)\n",
    "x = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()\n",
    "\n",
    "# 'binary_crossentropy'\n",
    "model.compile(loss=f1_loss, \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', f1]) \n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[checkpointer, checkpointer1, lrate_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load best model\n",
    "model.load_weights(checkpoint_path)\n",
    "y_pred = model.predict(x_val, batch_size=1)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Neural Network f1_sklearn: {}\".format(f1_score(y_val, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
