{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train deep model + own emmbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiancorrea/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    Embedding,\n",
    "    SpatialDropout1D,\n",
    ")\n",
    "from tensorflow.keras.models import (\n",
    "    Model,\n",
    "    load_model,\n",
    "    Sequential\n",
    ")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/pandas_data_frame.csv', index_col=0)\n",
    "all_data = df.where((pd.notnull(df)), '')\n",
    "all_data['hashtag'] = all_data['hashtag'].apply(literal_eval)\n",
    "\n",
    "full_text = all_data['tidy_tweet'][(all_data['label']==1.0) | (all_data['label']==0.0)]\n",
    "y = all_data['label'][(all_data['label']==1.0) | (all_data['label']==0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25569, 100) (25569,)\n",
      "(6393, 100) (6393,)\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(lower=True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(full_text)\n",
    "max_len = 100\n",
    "X = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, random_state=1992, test_size=0.2)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('../model_wehigts/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "# with open('tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 150)          2275050   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 150)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               280800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 2,556,051\n",
      "Trainable params: 2,556,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiancorrea/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/200\n",
      "25569/25569 [==============================] - 137s 5ms/step - loss: 0.2621 - acc: 0.9266 - f1: 0.0693 - val_loss: 0.2485 - val_acc: 0.9312 - val_f1: 0.0594\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24848, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00001: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 2/200\n",
      "25569/25569 [==============================] - 130s 5ms/step - loss: 0.2441 - acc: 0.9295 - f1: 0.0858 - val_loss: 0.2282 - val_acc: 0.9312 - val_f1: 0.0865\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24848 to 0.22817, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00002: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 3/200\n",
      "25569/25569 [==============================] - 127s 5ms/step - loss: 0.2172 - acc: 0.9319 - f1: 0.1514 - val_loss: 0.1995 - val_acc: 0.9352 - val_f1: 0.1858\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22817 to 0.19949, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00003: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 4/200\n",
      "25569/25569 [==============================] - 139s 5ms/step - loss: 0.1968 - acc: 0.9348 - f1: 0.2280 - val_loss: 0.1925 - val_acc: 0.9387 - val_f1: 0.2351\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19949 to 0.19245, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00004: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 5/200\n",
      "25569/25569 [==============================] - 129s 5ms/step - loss: 0.1732 - acc: 0.9399 - f1: 0.3010 - val_loss: 0.1918 - val_acc: 0.9395 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.19245 to 0.19181, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00005: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 6/200\n",
      "25569/25569 [==============================] - 133s 5ms/step - loss: 0.1541 - acc: 0.9456 - f1: 0.3749 - val_loss: 0.1874 - val_acc: 0.9398 - val_f1: 0.3355\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.19181 to 0.18738, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00006: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 7/200\n",
      "25569/25569 [==============================] - 135s 5ms/step - loss: 0.1369 - acc: 0.9508 - f1: 0.4331 - val_loss: 0.1858 - val_acc: 0.9423 - val_f1: 0.3687\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.18738 to 0.18576, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00007: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 8/200\n",
      "25569/25569 [==============================] - 134s 5ms/step - loss: 0.1263 - acc: 0.9551 - f1: 0.4756 - val_loss: 0.1782 - val_acc: 0.9449 - val_f1: 0.3687\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18576 to 0.17818, saving model to ../model_wehigts/5_w_bcl.hdf5\n",
      "\n",
      "Epoch 00008: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 9/200\n",
      "25569/25569 [==============================] - 138s 5ms/step - loss: 0.1166 - acc: 0.9595 - f1: 0.5173 - val_loss: 0.1908 - val_acc: 0.9345 - val_f1: 0.3887\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00009: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 10/200\n",
      "25569/25569 [==============================] - 139s 5ms/step - loss: 0.1106 - acc: 0.9601 - f1: 0.5395 - val_loss: 0.1922 - val_acc: 0.9412 - val_f1: 0.3289\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00010: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 11/200\n",
      "25569/25569 [==============================] - 137s 5ms/step - loss: 0.1022 - acc: 0.9635 - f1: 0.5673 - val_loss: 0.2155 - val_acc: 0.9252 - val_f1: 0.4016\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00011: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 12/200\n",
      "25569/25569 [==============================] - 131s 5ms/step - loss: 0.1048 - acc: 0.9618 - f1: 0.5669 - val_loss: 0.1940 - val_acc: 0.9366 - val_f1: 0.3951\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00012: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 13/200\n",
      "25569/25569 [==============================] - 129s 5ms/step - loss: 0.0947 - acc: 0.9658 - f1: 0.6112 - val_loss: 0.2026 - val_acc: 0.9426 - val_f1: 0.4385\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00013: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 14/200\n",
      "25569/25569 [==============================] - 127s 5ms/step - loss: 0.0853 - acc: 0.9705 - f1: 0.6571 - val_loss: 0.2015 - val_acc: 0.9351 - val_f1: 0.4472\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00014: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 15/200\n",
      "25569/25569 [==============================] - 124s 5ms/step - loss: 0.0803 - acc: 0.9713 - f1: 0.6641 - val_loss: 0.2142 - val_acc: 0.9288 - val_f1: 0.4459\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.17818\n",
      "\n",
      "Epoch 00015: saving model to ../model_wehigts/5_ch_bcl.hdf5\n",
      "Epoch 16/200\n",
      " 3840/25569 [===>..........................] - ETA: 1:38 - loss: 0.0806 - acc: 0.9708 - f1: 0.6866"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from personal_library.sce_keras.loss_functions import f1_loss\n",
    "from personal_library.sce_keras.metrics_functions import f1\n",
    "from personal_library.sce_keras.callbacks import (\n",
    "    LearningRateDecay,\n",
    "    WarmUpCosineDecayScheduler\n",
    ")\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "embed_dim = 150\n",
    "lstm_out = 200\n",
    "max_fatures = X.max() + 1\n",
    "learnRate = 0.001\n",
    "warmup_epoch = 20\n",
    "\n",
    "lrate_decay = LearningRateDecay(epochs, learnRate).step_decay\n",
    "warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=learnRate,\n",
    "                                        warmup_learning_rate=0,\n",
    "                                        warmup_epoch=warmup_epoch,\n",
    "                                        hold_base_rate_steps=5,\n",
    "                                        verbose=0)\n",
    "\n",
    "checkpoint_path = \"../model_wehigts/5_w_bcl.hdf5\"\n",
    "checkpoint_path1 = \"../model_wehigts/5_ch_bcl.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=True, mode='min')\n",
    "checkpointer1 = ModelCheckpoint(filepath=checkpoint_path1,\n",
    "                               monitor='val_loss', verbose=2,\n",
    "                               save_best_only=False, mode='min')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(lstm_out, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics = ['accuracy', f1])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[checkpointer, checkpointer1, lrate_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own emmbeding f1_sklearn: 0.5480427046263345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Load best model\n",
    "model.load_weights(checkpoint_path)\n",
    "y_pred = model.predict(x_val, batch_size=1)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Own emmbeding f1_sklearn: {}\".format(f1_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
